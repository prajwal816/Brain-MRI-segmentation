{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":407317,"datasetId":181273,"databundleVersionId":422498}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LGG Brain MRI Segmentation — Transfer Learning + Self-Attention (ResNet-U-Net)\n\nThis notebook trains a segmentation model on the LGG FLAIR abnormality dataset:\n- Patient-level split to prevent leakage\n- Transfer learning (ImageNet pretrained ResNet34 encoder)\n- Bottleneck self-attention (multi-head attention on deep features)\n- Dice + BCE loss, Dice/IoU metrics\n- Paper-style plots (global matplotlib styling)\n\nOutputs saved to `/kaggle/working/`.\n","metadata":{}},{"cell_type":"code","source":"# If Kaggle already has these, this cell is harmless.\n!pip -q install albumentations==1.3.1 opencv-python-headless==4.9.0.80\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, re, glob, random, math, time\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision.models import resnet34, ResNet34_Weights\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# ✅ GLOBAL PAPER-STYLE SETTINGS (as you provided)\n# -------------------------------\nplt.rcParams.update({\n    \"font.family\": \"serif\",\n    \"font.serif\": [\"Times New Roman\", \"Times\", \"DejaVu Serif\"],\n    \"font.size\": 12,\n\n    \"axes.labelsize\": 13,\n    \"axes.titlesize\": 13,\n    \"axes.linewidth\": 1.2,\n\n    \"xtick.labelsize\": 11,\n    \"ytick.labelsize\": 11,\n    \"xtick.major.size\": 6,\n    \"ytick.major.size\": 6,\n    \"xtick.minor.size\": 3,\n    \"ytick.minor.size\": 3,\n\n    \"legend.fontsize\": 11,\n    \"legend.frameon\": True,\n    \"legend.edgecolor\": \"0.4\",\n\n    \"grid.linestyle\": \":\",\n    \"grid.linewidth\": 0.7,\n    \"grid.alpha\": 0.85,\n})\n\ndef paper_axes(ax):\n    ax.minorticks_on()\n    ax.grid(True, which=\"major\", linestyle=\":\", linewidth=0.8)\n    ax.grid(True, which=\"minor\", linestyle=\":\", linewidth=0.5, alpha=0.7)\n\n    for spine in ax.spines.values():\n        spine.set_linewidth(1.2)\n\n    ax.tick_params(which=\"both\", direction=\"in\", top=True, right=True)\n\n# -------------------------------\n# Reproducibility\n# -------------------------------\nSEED = 7\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ntorch.backends.cudnn.deterministic = False  # faster; OK with fixed seed\ntorch.backends.cudnn.benchmark = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CFG = {\n    \"img_size\": 256,\n    \"batch_size\": 16,\n    \"num_workers\": 2,\n    \"epochs\": 12,\n    \"lr\": 3e-4,\n    \"weight_decay\": 1e-4,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"use_attention\": True,          # ablation knob\n    \"attn_heads\": 8,                # 512 channels / 8 heads works cleanly\n    \"threshold\": 0.5,\n}\n\nCFG\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Load dataset index (with safe fallback)\n\nPreferred:\n- Load `lgg_master_slices.csv` created by your EDA notebook.\n\nFallback:\n- Rebuild the dataframe directly from `/kaggle/input/**/kaggle_3m`.\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KAGGLE_INPUT = Path(\"/kaggle/input\")\ncandidates = list(KAGGLE_INPUT.glob(\"**/kaggle_3m\"))\nprint(\"Found candidates:\", [str(p) for p in candidates[:10]])\n\nif len(candidates) == 0:\n    raise FileNotFoundError(\"Could not find 'kaggle_3m' under /kaggle/input. Attach the dataset to this notebook.\")\n\nDATA_ROOT = candidates[0]\nDATA_ROOT\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_key(path):\n    base = Path(path).name\n    if base.endswith(\"_mask.tif\"):\n        base = base.replace(\"_mask.tif\", \"\")\n    else:\n        base = base.replace(\".tif\", \"\")\n    return base\n\ndef patient_id_from_path(path):\n    return Path(path).parent.name\n\ndef slice_index_from_key(k):\n    m = re.search(r\"_(\\d+)$\", k)\n    return int(m.group(1)) if m else np.nan\n\nEDA_CSV = Path(\"/kaggle/working/eda_outputs/lgg_master_slices.csv\")\n\nif EDA_CSV.exists():\n    df = pd.read_csv(EDA_CSV)\n    print(\"Loaded:\", EDA_CSV, \"| rows:\", len(df), \"| patients:\", df[\"patient_id\"].nunique())\nelse:\n    all_tifs = sorted(glob.glob(str(DATA_ROOT / \"*\" / \"*.tif\")))\n    mask_tifs = sorted([p for p in all_tifs if p.endswith(\"_mask.tif\")])\n    img_tifs  = sorted([p for p in all_tifs if not p.endswith(\"_mask.tif\")])\n\n    img_map = {to_key(p): p for p in img_tifs}\n    msk_map = {to_key(p): p for p in mask_tifs}\n    keys = sorted(set(img_map.keys()) & set(msk_map.keys()))\n\n    df = pd.DataFrame({\n        \"key\": keys,\n        \"image_path\": [img_map[k] for k in keys],\n        \"mask_path\":  [msk_map[k] for k in keys],\n    })\n    df[\"patient_id\"] = df[\"image_path\"].apply(patient_id_from_path)\n    df[\"slice_idx\"] = df[\"key\"].apply(slice_index_from_key)\n\n    print(\"Built df | rows:\", len(df), \"| patients:\", df[\"patient_id\"].nunique())\n\ndf.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if \"split\" not in df.columns or df[\"split\"].isna().any():\n    patients = df[\"patient_id\"].drop_duplicates().tolist()\n    rng = np.random.default_rng(SEED)\n    rng.shuffle(patients)\n\n    n = len(patients)\n    train_pat = set(patients[: int(0.8*n)])\n    val_pat   = set(patients[int(0.8*n): int(0.9*n)])\n    test_pat  = set(patients[int(0.9*n):])\n\n    def assign_split(pid):\n        if pid in train_pat: return \"train\"\n        if pid in val_pat: return \"val\"\n        return \"test\"\n\n    df[\"split\"] = df[\"patient_id\"].apply(assign_split)\n\ndf[\"split\"].value_counts(), df.groupby(\"split\")[\"patient_id\"].nunique()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset + augmentations\n\nNotes:\n- Images are 256×256 and stored as 3-channel uint8 (we normalize to [0,1]).\n- Masks are binarized to {0,1} and returned as a 1×H×W tensor.\n","metadata":{}},{"cell_type":"code","source":"train_tfms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.08, rotate_limit=12, p=0.6, border_mode=0),\n    A.RandomBrightnessContrast(p=0.25),\n    A.GaussianBlur(blur_limit=3, p=0.15),\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),  # keep simple; MR intensity isn't ImageNet-like\n    ToTensorV2(),\n])\n\nval_tfms = A.Compose([\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n    ToTensorV2(),\n])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_tif(path):\n    return np.array(Image.open(path))\n\ndef to_mask01(msk):\n    if msk.ndim == 3:\n        m = msk[..., 0]\n    else:\n        m = msk\n    return (m > 0).astype(np.uint8)\n\nclass LGGSegDataset(Dataset):\n    def __init__(self, df, tfms=None):\n        self.df = df.reset_index(drop=True)\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        r = self.df.iloc[idx]\n        img = load_tif(r.image_path)          # (H,W,3) uint8\n        msk = to_mask01(load_tif(r.mask_path))# (H,W) uint8 {0,1}\n\n        # Ensure 3 channels\n        if img.ndim == 2:\n            img = np.stack([img, img, img], axis=-1)\n\n        # Albumentations expects mask HxW or HxWx1\n        if self.tfms is not None:\n            out = self.tfms(image=img, mask=msk)\n            img_t = out[\"image\"]  # (3,H,W) float\n            msk_t = out[\"mask\"]   # (H,W)\n        else:\n            img = img.astype(np.float32) / 255.0\n            img_t = torch.from_numpy(img).permute(2,0,1).float()\n            msk_t = torch.from_numpy(msk).long()\n\n        msk_t = msk_t.unsqueeze(0).float()   # (1,H,W)\n        return img_t, msk_t\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = df[df[\"split\"]==\"train\"].copy()\nval_df   = df[df[\"split\"]==\"val\"].copy()\ntest_df  = df[df[\"split\"]==\"test\"].copy()\n\ntrain_ds = LGGSegDataset(train_df, tfms=train_tfms)\nval_ds   = LGGSegDataset(val_df, tfms=val_tfms)\ntest_ds  = LGGSegDataset(test_df, tfms=val_tfms)\n\ntrain_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\nval_loader   = DataLoader(val_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True)\n\nlen(train_ds), len(val_ds), len(test_ds)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xb, yb = next(iter(train_loader))\nprint(\"x:\", xb.shape, xb.dtype, \"y:\", yb.shape, yb.dtype, \"mask unique:\", torch.unique(yb))\n\n# Plot a few samples (paper style)\nfig, ax = plt.subplots(figsize=(13, 6))\npaper_axes(ax)\nax.axis(\"off\")\nax.set_title(\"Training batch sanity-check (first sample)\")\n\nimg = xb[0].permute(1,2,0).cpu().numpy()\nmsk = yb[0,0].cpu().numpy()\n\nax.imshow(np.clip(img, 0, 1))\nax.imshow(msk, alpha=0.35, cmap=\"Reds\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model: ResNet34 U-Net + bottleneck self-attention\n\n- Encoder: ResNet34 pretrained on ImageNet (transfer learning)\n- Bottleneck attention: multi-head self-attention over the 8×8 deep feature tokens\n- Decoder: U-Net style upsampling with skip connections\n","metadata":{}},{"cell_type":"code","source":"class ConvBNReLU(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, p=1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=k, padding=p, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass UpBlock(nn.Module):\n    def __init__(self, in_ch, skip_ch, out_ch):\n        super().__init__()\n        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n        self.conv1 = ConvBNReLU(out_ch + skip_ch, out_ch)\n        self.conv2 = ConvBNReLU(out_ch, out_ch)\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        # handle any off-by-1 due to rounding (shouldn't happen at 256, but keep safe)\n        if x.shape[-2:] != skip.shape[-2:]:\n            x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n        x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\nclass BottleneckMHSA(nn.Module):\n    \"\"\"\n    Multi-head self-attention on deep feature map.\n    Operates on (B,C,H,W) -> tokens (B,HW,C) -> MHA -> reshape back, residual + FFN.\n    \"\"\"\n    def __init__(self, channels, heads=8, dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(channels)\n        self.attn = nn.MultiheadAttention(embed_dim=channels, num_heads=heads,\n                                          dropout=dropout, batch_first=True)\n        self.norm2 = nn.LayerNorm(channels)\n        self.ffn = nn.Sequential(\n            nn.Linear(channels, channels*4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(channels*4, channels),\n        )\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        tokens = x.permute(0,2,3,1).reshape(b, h*w, c)  # (B,HW,C)\n\n        t = self.norm1(tokens)\n        attn_out, _ = self.attn(t, t, t, need_weights=False)\n        tokens = tokens + attn_out\n\n        t2 = self.norm2(tokens)\n        tokens = tokens + self.ffn(t2)\n\n        x_out = tokens.reshape(b, h, w, c).permute(0,3,1,2)\n        return x_out\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNet34UNetAttn(nn.Module):\n    def __init__(self, pretrained=True, use_attention=True, attn_heads=8):\n        super().__init__()\n        weights = ResNet34_Weights.IMAGENET1K_V1 if pretrained else None\n        self.encoder = resnet34(weights=weights)\n\n        # ResNet stem\n        self.conv1 = self.encoder.conv1\n        self.bn1   = self.encoder.bn1\n        self.relu  = self.encoder.relu\n        self.maxp  = self.encoder.maxpool\n\n        # Encoder stages (skip connections)\n        self.layer1 = self.encoder.layer1  # 64\n        self.layer2 = self.encoder.layer2  # 128\n        self.layer3 = self.encoder.layer3  # 256\n        self.layer4 = self.encoder.layer4  # 512\n\n        self.use_attention = use_attention\n        self.attn = BottleneckMHSA(512, heads=attn_heads) if use_attention else nn.Identity()\n\n        # Decoder\n        self.up3 = UpBlock(512, 256, 256)\n        self.up2 = UpBlock(256, 128, 128)\n        self.up1 = UpBlock(128, 64, 64)\n\n        # Stem skip: after conv1+bn+relu is 64 at 128x128\n        self.up0 = UpBlock(64, 64, 64)\n\n        self.out = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        x0 = self.relu(self.bn1(self.conv1(x)))  # (B,64,128,128)\n        x1 = self.layer1(self.maxp(x0))          # (B,64,64,64)\n        x2 = self.layer2(x1)                     # (B,128,32,32)\n        x3 = self.layer3(x2)                     # (B,256,16,16)\n        x4 = self.layer4(x3)                     # (B,512,8,8)\n\n        # Self-attention bottleneck\n        x4 = self.attn(x4)\n\n        # Decoder (upsample + skips)\n        d3 = self.up3(x4, x3)                    # (B,256,16,16)\n        d2 = self.up2(d3, x2)                    # (B,128,32,32)\n        d1 = self.up1(d2, x1)                    # (B,64,64,64)\n        d0 = self.up0(d1, x0)                    # (B,64,128,128)\n\n        # Final upsample to 256x256\n        d0 = F.interpolate(d0, scale_factor=2, mode=\"bilinear\", align_corners=False)  # (B,64,256,256)\n\n        logits = self.out(d0)                    # (B,1,256,256)\n        return logits\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dice_coeff(probs, targets, eps=1e-7):\n    # probs, targets: (B,1,H,W) float in [0,1]\n    num = 2.0 * (probs * targets).sum(dim=(2,3))\n    den = (probs + targets).sum(dim=(2,3)) + eps\n    return (num / den).mean()\n\ndef iou_coeff(probs, targets, eps=1e-7):\n    inter = (probs * targets).sum(dim=(2,3))\n    union = (probs + targets - probs*targets).sum(dim=(2,3)) + eps\n    return (inter / union).mean()\n\nclass DiceLoss(nn.Module):\n    def __init__(self, eps=1e-7):\n        super().__init__()\n        self.eps = eps\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        num = 2.0 * (probs * targets).sum(dim=(2,3))\n        den = (probs + targets).sum(dim=(2,3)) + self.eps\n        dice = (num / den)\n        return 1.0 - dice.mean()\n\nbce = nn.BCEWithLogitsLoss()\n\ndef total_loss(logits, targets):\n    return 0.6 * bce(logits, targets) + 0.4 * DiceLoss()(logits, targets)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, loader, device):\n    model.eval()\n    losses, dices, ious = [], [], []\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        logits = model(xb)\n        loss = total_loss(logits, yb)\n\n        probs = torch.sigmoid(logits)\n        probs_bin = (probs > CFG[\"threshold\"]).float()\n\n        losses.append(loss.item())\n        dices.append(dice_coeff(probs_bin, yb).item())\n        ious.append(iou_coeff(probs_bin, yb).item())\n\n    return {\n        \"loss\": float(np.mean(losses)),\n        \"dice\": float(np.mean(dices)),\n        \"iou\":  float(np.mean(ious)),\n    }\n\ndef train_one_epoch(model, loader, optimizer, scaler, device):\n    model.train()\n    losses = []\n    for xb, yb in loader:\n        xb = xb.to(device, non_blocking=True)\n        yb = yb.to(device, non_blocking=True)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n            logits = model(xb)\n            loss = total_loss(logits, yb)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        losses.append(loss.item())\n\n    return float(np.mean(losses))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = CFG[\"device\"]\nmodel = ResNet34UNetAttn(\n    pretrained=True,\n    use_attention=CFG[\"use_attention\"],\n    attn_heads=CFG[\"attn_heads\"],\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\nscaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n\nbest_val_dice = -1.0\nhistory = []\n\nsave_path = Path(\"/kaggle/working/best_resnet34_unet_attn.pt\")\n\nfor epoch in range(1, CFG[\"epochs\"]+1):\n    t0 = time.time()\n    train_loss = train_one_epoch(model, train_loader, optimizer, scaler, device)\n    val_stats = evaluate(model, val_loader, device)\n\n    row = {\n        \"epoch\": epoch,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_stats[\"loss\"],\n        \"val_dice\": val_stats[\"dice\"],\n        \"val_iou\": val_stats[\"iou\"],\n        \"sec\": time.time() - t0\n    }\n    history.append(row)\n\n    if val_stats[\"dice\"] > best_val_dice:\n        best_val_dice = val_stats[\"dice\"]\n        torch.save({\"model\": model.state_dict(), \"cfg\": CFG}, save_path)\n\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_stats['loss']:.4f} | \"\n          f\"val_dice={val_stats['dice']:.4f} | val_iou={val_stats['iou']:.4f} | best_dice={best_val_dice:.4f}\")\n\nhistory_df = pd.DataFrame(history)\nhistory_df.tail()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}